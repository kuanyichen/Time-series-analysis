---
title: "Modeling and Forecasting House Price Trends"
author: "Kuanyi Chen"
date: "2025-01-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE, warning=FALSE}
# import necessary packages
library(tidyverse)
library(ggplot2)
library(forecast)
library(rsq)
```

# I. Introduction

```{r}
# import data
data <- read.csv("house_property_sales.csv")

# filter data to only include houses with 2 bedrooms
data <- data %>% 
  filter(bedrooms == 2 & type == "house")
```

This time series data is the house property sales data for 2 bedroom houses for a specific region (region is not indicated in the source), recorded quarterly from 9/30/2007 to 9/30/2019. The data is the moving average of median house price, which is calculated by taking the average of median house prices over a consecutive period of time, updated as new data points are added. This is a useful time series data, as house prices are relevant to sellers and buyers in the market. It will give people an idea of the trends and changes in the market.

# II. Results

## 1. Modeling and Forecasting Trend

### (a)

```{r}
# turn data into a time series
data_ts <- ts(data$MA, start = c(2007, 3), frequency = 4)

# plot the time series
autoplot(data_ts) + 
  labs(title = "Moving Average of Median House Price",
       y = "Price (USD)")
```

### (b)

The plot in (a) does not suggest the data is covariance stationary. In order to be covariance stationary, a time series must satisify three conditions: constant mean, constant variance, and time independent covariances. Since the plot has an overall increasing trend, it does not have constant mean, thus not covariance stationary.

### (c)

```{r}
# plot the ACF and PACF plot next to each other
par(mfrow=c(1,2))
acf(data_ts)
pacf(data_ts)
```
The ACF plot showcases a gradual decline in the bars, with the autocorrelation being significant until a little bit past the second lag. This gradual decline in autocorrelation means the autocorrelation between the time series and its lagged values gets weaker as the lag increases. This also means a long-term dependency in the data.

The PACF does not showcase significant autocorrelation at any lags except the zeroth lag, so it is not necessary to include any lags to address autocorrelation.

### (d) (e) (f) (g)

```{r}
# extract the time of the time series and convert into numeric type
t <- as.numeric(time(data_ts))
```

#### Linear Model

```{r, fig.align="center"}
# fit and plot a linear model
linear_m <- lm(data_ts ~ t)
plot(data_ts, ylab = "Price (USD)", main = "Moving Average of Median House Price")
abline(linear_m, col = "red")
```

```{r}
# residuals vs fitted values plot
plot(linear_m$fitted.values, linear_m$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")
```
The residuals showcase an up and down pattern, similar to a sine and cosine wave. This indicates some seasonality in the data, as the data fluctuates along the growing trend.

```{r}
# histogram of residuals
hist(linear_m$residuals,
     main = "Histogram of Residuals",
     xlab = "Residuals")
```
The residuals histogram roughly follows a normal distribution with some values present on the further left negative side, which may be an outlier.

```{r}
# diagnostics
summary(linear_m)
```
The adjusted $R^2$ is 0.8879, meaning linear model explains the variability in the data well. According to p-value of the t-distribution, both the coefficient and intercept are significant at the 5% significance level. According to the p-value of the F-distribution, the linear model is significant in fitting the data. 

#### Quadratic Model

```{r, fig.align="center"}
# fit and plot a quadratic model
quadratic_m <- lm(data_ts ~ t + I(t^2))
plot(data_ts, ylab = "Price (USD)", main = "Moving Average of Median House Price")
lines(t, quadratic_m$fitted.values, col = "red")
```

```{r}
# residuals vs fitted values plot
plot(quadratic_m$fitted.values, quadratic_m$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")
```
The residuals showcase an up and down pattern, similar to a sine and cosine wave. This indicates some seasonality in the data, as the data fluctuates along the growing trend.

```{r}
# histogram of residuals
hist(quadratic_m$residuals,
     main = "Histogram of Residuals",
     xlab = "Residuals")
```
The residuals histogram roughly follows a normal distribution and is left skewed. Some residuals on the further left may potentially be outliers.

```{r}
# diagnostics
summary(quadratic_m)
```
The adjusted $R^2$ is 0.898, meaning quadratic model explains the variability in the data well. According to p-value of the t-distribution, both the coefficient and intercept are significant at the 5% significance level. According to the p-value of the F-distribution, the quadratic model is significant in fitting the data. 

#### Log-linear Model

```{r, fig.align="center"}
# fit and plot a log-linear model
log_linear_m <- lm(log(data_ts) ~ t)
plot(log(data_ts), ylab = "log(Price) (USD)", main = "Moving Average of Median House Price (log)")
lines(t, log_linear_m$fitted.values, col = "red")
```

```{r}
# residuals vs fitted values plot
plot(log_linear_m$fitted.values, log_linear_m$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")
```
The residuals showcase an up and down pattern, similar to a sine and cosine wave. This indicates some seasonality in the data, as the data fluctuates along the growing trend.

```{r}
# histogram of residuals
hist(log_linear_m$residuals,
     main = "Histogram of Residuals",
     xlab = "Residuals")
```
The residuals histogram roughly follows a normal distribution, with some residuals on the further left on the negative side, which may potentially be outliers

```{r}
# diagnostics
summary(log_linear_m)
```
The adjusted $R^2$ is 0.8891, meaning log-linear model explains the variability in the data well. According to p-value of the t-distribution, both the coefficient and intercept are significant at the 5% significance level. According to the p-value of the F-distribution, the log-linear model is significant in fitting the data.

#### Exponential Model

```{r, fig.align="center"}
# fit and plot an exponential model
data_ts_df <- data.frame(x = t, y = data_ts)
exponential_m <- nls(y ~ exp(a + b * t), data = data_ts_df, 
                     start = list(a = log(min(data_ts)), b = 0.01))
plot(data_ts, ylab = "Price (USD)", main = "Moving Average of Median House Price")
lines(t, predict(exponential_m, list(x = t)), col = "red")
```

```{r}
# residuals vs fitted values plot
plot(fitted.values(exponential_m), residuals(exponential_m), 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")
```
The residuals showcase an up and down pattern, similar to a sine and cosine wave. This indicates some seasonality in the data, as the data fluctuates along the growing trend.

```{r}
# histogram of residuals
hist(residuals(exponential_m),
     main = "Histogram of Residuals",
     xlab = "Residuals")
```
The residuals histogram roughly follows a normal distribution with some values present on the further left negative side, which may be an outlier.


```{r}
# diagnostics
summary(exponential_m)

# MSE
MSE <- sum((data_ts - fitted(exponential_m))^2) / length(data_ts)
MSE
```
Since the exponential regression is not linear, $R^2$ is not appropriate here. Instead, we assess the model by its mean squared error (MSE). The MSE of the exponential fit is 50087470, which is a high value, indicating exponential may not be a good fit for the data. According to p-value of the t-distribution, both the `a` and `b` are significant at the 5% significance level.

#### Log-quadratic 

```{r, fig.align="center"}
# fit and plot a log-quadratic model
log_quadratic_m <- lm(log(data_ts) ~ t + I(t^2))
plot(log(data_ts), ylab = "log(Price) (USD)", main = "Moving Average of Median House Price (log)")
lines(t, log_quadratic_m$fitted.values, col = "red")
```

```{r}
# residuals vs fitted values plot
plot(log_quadratic_m$fitted.values, log_quadratic_m$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")
```
The residuals showcase an up and down pattern, similar to a sine and cosine wave. This indicates some seasonality in the data, as the data fluctuates along the growing trend.

```{r}
# histogram of residuals
hist(log_quadratic_m$residuals,
     main = "Histogram of Residuals",
     xlab = "Residuals")
```
The residuals histogram roughly follows a normal distribution and is left skewed. Some residuals on the further left may potentially be outliers.

```{r}
# diagnostics
summary(log_quadratic_m)
```
The adjusted $R^2$ is 0.8956, meaning log quadratic model explains the variability in the data well. According to p-value of the t-distribution, none of the coefficient and intercept are significant at the 5% significance level, but are significant at the 10% significance level. However, according to the p-value of the F-distribution, the log quadratic model is significant in fitting the data. 

#### Log-quadratic-periodic Model

```{r, fig.align="center"}
sin_t <- sin(2 * pi * t)
cos_t <- cos(2 * pi * t)
log_quadratic_periodic_m <- lm(log(data_ts) ~ t + I(t^2) + sin_t + cos_t)
plot(log(data_ts), ylab = "log(Price) (USD)", main = "Moving Average of Median House Price (log)")
lines(t, log_quadratic_periodic_m$fitted.values, col = "red")
```

```{r}
# residuals vs fitted values plot
plot(log_quadratic_periodic_m$fitted.values, log_quadratic_periodic_m$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")
```
The residuals showcase an up and down pattern, similar to a sine and cosine wave. This indicates some seasonality in the data, as the data fluctuates along the growing trend.

```{r}
# histogram of residuals
hist(log_quadratic_periodic_m$residuals,
     main = "Histogram of Residuals",
     xlab = "Residuals")
```
The residuals histogram roughly follows a normal distribution and is left skewed. Some residuals on the further left may potentially be outliers.

```{r}
# diagnostics
summary(log_quadratic_periodic_m)
```
The adjusted $R^2$ is 0.8911, meaning log quadratic periodic model explains the variability in the data well. According to p-value of the t-distribution, none of the coefficient and intercept are significant at the 5% significance level, but the intercept, t, and $t^2$ are significant at the 10% significance level. According to the p-value of the F-distribution, the log quadratic periodic model is significant in fitting the data. 

#### Log-linear-periodic Model

```{r, fig.align="center"}
sin_t <- sin(2 * pi * t)
cos_t <- cos(2 * pi * t)
log_linear_periodic_m <- lm(log(data_ts) ~ t + sin_t + cos_t)
plot(log(data_ts), ylab = "log(Price) (USD)", main = "Moving Average of Median House Price (log)")
lines(t, log_linear_periodic_m$fitted.values, col = "red")
```

```{r}
# residuals vs fitted values plot
plot(log_linear_periodic_m$fitted.values, log_linear_periodic_m$residuals, 
     main = "Residuals vs Fitted Values",
     xlab = "Fitted values",
     ylab = "Residuals")
abline(h = 0, col = "red")
```
The residuals showcase an up and down pattern, similar to a sine and cosine wave. This indicates some seasonality in the data, as the data fluctuates along the growing trend.

```{r}
# histogram of residuals
hist(log_linear_periodic_m$residuals,
     main = "Histogram of Residuals",
     xlab = "Residuals")
```
The residuals histogram roughly follows a normal distribution with some values present on the further left negative side, which may be an outlier.

```{r}
# diagnostics
summary(log_linear_periodic_m)
```
The adjusted $R^2$ is 0.8845, meaning log quadratic periodic model explains the variability in the data well. According to p-value of the t-distribution, the coefficient of t and intercept are significant at the 5% significance level. According to the p-value of the F-distribution, the log linear periodic model is significant in fitting the data. 

### (h)

```{r}
AIC(linear_m, log_linear_m, quadratic_m, log_quadratic_m, exponential_m, 
    log_quadratic_periodic_m, log_linear_periodic_m)
BIC(linear_m, log_linear_m, quadratic_m, log_quadratic_m, exponential_m, 
    log_quadratic_periodic_m, log_linear_periodic_m) 
```
Both AIC and BIC indicate that the log quadratic model is the most suitable.

### (i)

```{r}
# plot forecast for 24 steps (quarters) ahead
tn <- data.frame(t = seq(from = 2019.75, length.out = 24, by = 0.25))
pred <- predict(lm(log(data_ts) ~ t + I(t^2)), tn, se.fit = TRUE)
plot(c(time(data_ts), tn$t), c(log(data_ts), pred$fit), type = 'l', xlim = c(2007, 2026), 
     ylab = "Price (USD)", xlab = "Time", main = "Moving Average of Median House Price (log)")
lines(tn$t, pred$fit, col = "red")

# compute the 95% confidence and prediction intervals
pred.plim <- predict(lm(log(data_ts) ~ t + I(t^2)), tn, level = 0.95, interval = "prediction")
pred.clim <- predict(lm(log(data_ts) ~ t + I(t^2)), tn, level = 0.95, interval = "confidence")

# shade the confidence interval
polygon(c(tn$t, rev(tn$t)), 
        c(pred.clim[,3], rev(pred.clim[,2])), 
        col = rgb(0, 0, 1, 0.3), border = NA)

# shade the prediction interval
polygon(c(tn$t, rev(tn$t)), 
        c(pred.plim[,3], rev(pred.plim[,2])), 
        col = rgb(1, 0, 0, 0.2), border = NA)

# add legend
legend("bottomright", legend = c("Observed", "Forecasted", "95% Confidence Interval", 
                                 "95% Prediction Interval"), 
       col = c("black", "red", rgb(0, 0, 1, 0.3), rgb(1, 0, 0, 0.2)), lty = c(1, 1, NA, NA), 
       pch = c(NA, NA, 15, 15), cex = 0.8, inset = c(0.05, 0.05))


# plot confidence and prediction interval of forecast
matplot(tn$t, cbind(pred.clim, pred.plim[,-1]),
        lty = c(1,1,1,3,3), type = "l", lwd = 2, ylab = "Price (USD)", xlab = "Time", 
        main = "Forecast of Moving Average of Median House Price")

# add legend
legend("topleft", 
       legend = c("Forecast", "Lower 95% CI", "Upper 95% CI", "Lower 95% PI", "Upper 95% PI"), 
       lty = c(1, 1, 1, 3, 3), 
       lwd = 2, 
       col = 1:5, 
       cex = 0.8,
       inset = c(0.05, 0.05))
```

## 2. Trend and Seasonal Adjustments

### (a)

```{r}
# additive decomposition
additive_decomp <- decompose(data_ts, type = "additive")
par(mfrow=c(1,2))
acf(na.omit(additive_decomp$random), main = "ACF of Residuals")
pacf(na.omit(additive_decomp$random), main = "PACF of Residuals")
```
On the ACF, the residuals do not showcase autocorrelation across different lags. On the PACF, the residual is only significant at lag 1.25, while others are insignificant, meaning the residuals are mainly random white noise and do not have a regular pattern. 

### (b)

```{r}
# multiplicative decomposition
mult_decomp <- decompose(data_ts, type = "multiplicative")
par(mfrow=c(1,2))
acf(na.omit(mult_decomp$random), main = "ACF of Residuals")
pacf(na.omit(mult_decomp$random), main = "PACF of Residuals")
```
On the ACF, the residuals do not showcase autocorrelation across different lags. On the PACF, the residual is only significant at lag 0.5 and 1.25, while others are insignificant, meaning the residuals are mainly random white noise and do not have a regular pattern. 

### (c)

The ACF and PACF plots of the residuals of both additive and multiplicative decompositions are very similar. Both of the residuals of the additive and multiplicative methods are insignificant at most lags, indicating randomness and white noise. However, the PACF plot of the residuals of the additive decomposition has one less significant spike at a lag. So, additive is the better method here. Also, additive method is more straightforward to compute.

### (d)

The models for the additive and multiplicative cycles would be similar, since their ACF and PACF plots are very similar. This means the residuals of both methods are mostly random (white noise) and do not have significant correlations at almost all the lags.

### (e)

```{r}
# fit seasonal factors
fit <- tslm(log(data_ts) ~ season + 0)
```

```{r}
#  plot of coefficient of seasonal factors
plot(fit$coef, type = 'l', ylab = 'Seasonal Factors', xlab = "Season", lwd = 2, 
     main = "Plot of Seasonal Factors")
```
According to the plot of seasonal factors, there is an increase in seasonal effects from the season 1 to 2, and a peak at season 2, indicating high seasonal effects in the second quarter. After the second quarter, the seasonal effects decrease, with the fourth quarter having the lowest seasonal effects.

### (f)

```{r}
# fit and plot log quadratic model with seasonal component
log_quadratic_seasonal_m <- tslm(log(data_ts) ~ trend + I(trend^2) + season + 0)
autoplot(forecast(log_quadratic_seasonal_m, h = 12), ylab = "log(Price) (USD)", 
         main = "Forecast Trend + Seasonality")
```

# III. Conclusions and Future Work

### Conclusions:

The report identifies the log-quadratic model as the most suitable for forecasting house price trends, as supported by its superior performance based on both AIC and BIC values. The model provides a reasonable forecast with confidence and prediction intervals, capturing the long-term trend and seasonality in the data.

1. **Model Selection**: 
   - The log-quadratic model with seasonal adjustments performs best due to its ability to capture the time series data's trend while incorporating seasonality.

2. **Residual Analysis**:
   - Residual patterns indicate some periodic seasonality and possible outliers, but they mostly resemble white noise after incorporating seasonality, confirming the model's adequacy.

3. **Forecasting Results**:
   - The model generates a reliable forecast for 24 quarters ahead with well-defined confidence and prediction intervals. These intervals provide a clear range of potential outcomes.

### Future Work:
1. **Handling Residual Skewness and Outliers**:
   - The residual histogram showed slight skewness and potential outliers on the left tail. Removing or accounting for these outliers could refine the model's accuracy.

2. **Exploration of Alternative Models**:
   - Seasonal ARIMA or exponential smoothing methods could be tested to verify if they outperform the log-quadratic model in capturing both short-term fluctuations and long-term trends.

3. **Dynamic Seasonality**:
   - Seasonality was modeled additively, but exploring dynamic seasonal components might better capture variations in seasonal intensity over time.

4. **External Factors**:
   - Incorporating external variables, such as economic indicators (e.g., interest rates, inflation), could enhance the model's explanatory power.

5. **Regularization Techniques**:
   - To avoid overfitting, especially in periodic models with higher complexity, ridge regression or lasso could be applied.

# IV. References

Data Source: 

https://www.kaggle.com/datasets/htagholdings/property-sales?resource=download&select=ma_lga_12345.csv







