---
title: "Time Series Analysis Project 3"
author: "Kuanyi Chen"
date: "2025-03-11"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=F, message=FALSE, include=FALSE}
# import libraries
library(lattice)
library(ggplot2)
library(foreign)
library(MASS)
library(car)
require(stats)
require(stats4)
library(KernSmooth)
library(fastICA)
library(cluster)
library(fable)
library(leaps)
library(mgcv)
library(rpart)
library(forecastHybrid)
library(pan)
library(tsibble)
library(lubridate)
library(mgcv)
library(DAAG)
library(TTR)
library(tis)
library(prophet)
library(fable.prophet)
require("datasets")
require(graphics)
library(forecast)
library(xtable)
library(stats)
library(TSA)
library(timeSeries)
library(fBasics)
library(furrr)
library(tseries)
library(timsac)
library(TTR)
library(broom)
library(dplyr)
library(fpp)
library(fpp2)
library(fpp3)
library(future)
library(strucchange)
library(vars)
library(lmtest)
library(seasonal)
library(xts)
library(rugarch)
```

# I. Introduction

```{r}
# import and extract data
us_house_price <- read.csv("https://www.fhfa.gov/hpi/download/quarterly_datasets/hpi_at_state.csv")
names(us_house_price) <- c("State", "Year", "Quarter", "Price")
ca_price <- filter(us_house_price, State == "CA")

# create time series
ca_price_ts <- ts(ca_price$Price, start = c(1975, 1), frequency = 4)
```
The time series is the quarterly house price index (HPI) of California single-family houses from 1975 Q1 to 2024 Q2. HPI is a metric that measures changes in the prices of houses, with a certain time set as the base period with index value equal 100. In this data, the base period is 1980 Q1. We will fit different models to the data, including ARIMA, ETS, Holt-Winters, NNETAR, Prophet, and forecasting combination. We will perform different diagnostic tests on the models, including residuals vs. fitted plot, ACF and PACF of residuals, Ljung-Box test of residuals, and CUSUM plot of residuals. We will also identify a preferred model based on the training and testing errors.

# II. Results

```{r}
# plot the time series
autoplot(ca_price_ts) +
  ggtitle("California House Price Index (HPI)") +
  ylab("HPI")
```
The time series has an overall positive growing trend. There is an irregular cycle pattern, with a sudden spike aroun 2006 and 2007. There seems to be small seasonality, which is not clearly visible in the overall plot.

```{r}
# stl decomposition of time series
autoplot(stl(ca_price_ts, s.window = "periodic"))
```
The decomposed graph showcases a positive trend. It also showcases the presence of regular seasonality. There is also presence of some irregular noise.

```{r}
# plot acf and pacf
par(mfrow = c(1, 2))
acf(as.numeric(ca_price_ts))
pacf(as.numeric(ca_price_ts))
```
The ACF plot indicates strong correlation in the data, and the PACF indicates significance to the first lag to incorporate in an AR model.

### Split Training and Testing Data

We split the data into 70% training and 30% testing, which is the standard train test split proportion.

```{r}
# 70% training data, 30% testing data
ca_price_train <- subset(ca_price_ts, end = length(ca_price_ts) * 0.7)

ca_price_test <- subset(ca_price_ts, start = length(ca_price_ts) * 0.7 + 1)
```

### 1. ARIMA

```{r}
# auto.arima
auto_arima_model <- auto.arima(ca_price_ts)
```

```{r}
# plot residuals vs fitted values
plot(fitted(auto_arima_model), residuals(auto_arima_model), 
     main = "Auro.arima model residuals vs fitted values",
     xlab = "Fitted values",
     ylab = "Residuals")
```
The residuals are generally evenly scattered around 0. They are more clustered in the beginning and become more spread out as the fitted values increase.

```{r}
# plot ACF and PACF of residuals
par(mfrow = c(1, 2))
acf(as.numeric(residuals(auto_arima_model)))
pacf(as.numeric(residuals(auto_arima_model)))
```
The residuals are insignificant at all the lags, indicating no pattern or autocorrelation. The residuals are white noise.

```{r}
# ljung-box test on residuals
Box.test(residuals(auto_arima_model))
```
The p-value of the Ljung-box test is not significant at the 95% confidence level, so we fail to reject the null hypothesis and conclude that there is no autocorrelation in the residuals.

```{r}
# plot the CUSUM
plot(efp(residuals(auto_arima_model) ~ 1, type = "Rec-CUSUM"),
main = "Auto.arima CUSUM plot")
```
According to the CUSUM plot, the cumulative sum of residuals starts out with little fluctuation around 0, then the fluctuation starts to increase past 2000.

```{r}
# plot the 12 quarter ahead forecast
autoplot(forecast(auto_arima_model, h = 12)) +
  ggtitle("Auto.arima model 12 step ahead forecast") +
  ylab("HPI")
```

### 2. ETS

```{r}
# ets
ets_model <- ets(ca_price_ts)
```

```{r}
# plot residuals vs fitted values
plot(fitted(ets_model), residuals(ets_model), 
     main = "ETS model residuals vs fitted values",
     xlab = "Fitted values",
     ylab = "Residuals")
```
The residuals are generally evenly scattered around 0. They are more clustered in the beginning and become more spread out as the fitted values increase.

```{r}
# plot ACF and PACF of residuals
par(mfrow = c(1, 2))
acf(as.numeric(residuals(ets_model)))
pacf(as.numeric(residuals(ets_model)))
```
Most of the residuals are insignificant at the lags, except lag 3. So there is almost no pattern or autocorrelation, closely resembling white noise.

```{r}
# ljung-box test on residuals
Box.test(residuals(ets_model))
```
The p-value of the Ljung-box test is not significant at the 95% confidence level, so we fail to reject the null hypothesis and conclude that there is no autocorrelation in the residuals.

```{r}
# plot the CUSUM
plot(efp(residuals(ets_model) ~ 1, type = "Rec-CUSUM"),
main = "ETS CUSUM plot")
```
According to the CUSUM plot, the cumulative sum of residuals starts out with little fluctuation around 0, but then soon deviates towards the negative direction.

```{r}
# plot the 12 quarter ahead forecast
autoplot(forecast(ets_model, h = 12)) +
  ggtitle("ETS model 12 step ahead forecast") +
  ylab("HPI")
```

### 3. Holt-Winters

```{r}
# holt-winters method
holt_winters_model <- HoltWinters(ca_price_ts)
```

```{r}
# plot residuals vs fitted values
plot(fitted(holt_winters_model)[,1], residuals(holt_winters_model), 
     main = "Holt-Winters model residuals vs fitted values",
     xlab = "Fitted values",
     ylab = "Residuals")
```
The residuals are generally evenly scattered around 0. They are more clustered in the beginning and become more spread out as the fitted values increase.

```{r}
# plot ACF and PACF of residuals
par(mfrow = c(1, 2))
acf(as.numeric(residuals(holt_winters_model)))
pacf(as.numeric(residuals(holt_winters_model)))
```
Most of the residuals are insignificant at the lags, except lag 2, 4, and 6. So there is almost no pattern or autocorrelation, closely resembling white noise.

```{r}
# ljung-box test on residuals
Box.test(residuals(holt_winters_model))
```
The p-value of the Ljung-box test is not significant at the 95% confidence level, so we fail to reject the null hypothesis and conclude that there is no autocorrelation in the residuals.

```{r}
# plot the CUSUM
plot(efp(residuals(holt_winters_model) ~ 1, type = "Rec-CUSUM"),
main = "Holt-Winters CUSUM plot")
```
According to the CUSUM plot, the cumulative sum of residuals starts out with little fluctuation around 0, then has some minor fluctuations in the 2000s.

```{r}
# plot the 12 quarter ahead forecast
autoplot(forecast(holt_winters_model, h = 12)) +
  ggtitle("Holt-Winters model 12 step ahead forecast") +
  ylab("HPI")
```

### 4. NNETAR

```{r}
# NNETAR method
nnetar_model <- nnetar(ca_price_ts)
```

```{r}
# plot residuals vs fitted values
plot(fitted(nnetar_model), residuals(nnetar_model), 
     main = "NNETAR model residuals vs fitted values",
     xlab = "Fitted values",
     ylab = "Residuals")
```
The residuals are generally evenly scattered around 0. They are more clustered in the beginning and become more spread out as the fitted values increase.

```{r}
# plot ACF and PACF of residuals
par(mfrow = c(1, 2))
acf(as.numeric(na.omit(residuals(nnetar_model))))
pacf(as.numeric(na.omit(residuals(nnetar_model))))
```
There seems to be some pattern and autocorrelation in the residuals, as they are significant at multiple lags. So the residuals do not resemble white noise.

```{r}
# ljung-box test on residuals
Box.test(na.omit(residuals(nnetar_model)))
```
The p-value of the Ljung-box test is significant at the 95% confidence level, so we reject the null hypothesis and conclude that there is autocorrelation in the residuals, indicating a lack of fit of the model.

```{r}
# plot the CUSUM
plot(efp(na.omit(residuals(nnetar_model)) ~ 1, type = "Rec-CUSUM"),
main = "NNETAR CUSUM plot")
```
According to the CUSUM plot, the cumulative sum of residuals starts to deviate from 0 early on, but does not deviate in a particular positive or negative direction, instead stays around 0.

```{r}
# plot the 12 quarter ahead forecast
autoplot(forecast(nnetar_model, h = 12)) +
  ggtitle("NNETAR model 12 step ahead forecast") +
  ylab("HPI")
```

### 5. Prophet

```{r, message=FALSE}
# prophet method
ca_prices_tsibble <- tsibble(
  Quarter = yearquarter(seq(as.Date("1975-01-01"),
                            as.Date("2024-10-01"), by = "quarter")),
  Price = ca_price_ts)

prophet_model <- ca_prices_tsibble %>%
  model(prophet = prophet(Price ~ season(period = 4, order = 4,
                                    type = "additive")))
```

```{r}
# plot residuals vs fitted values
plot(fitted(prophet_model)[,3][[1]], residuals(prophet_model)[,3][[1]], 
     main = "Prophet model residuals vs fitted values",
     xlab = "Fitted values",
     ylab = "Residuals")
```
There is a strong pattern and autocorrelation in the residuals, and the residuals are not evenly scattered around 0.

```{r}
# plot ACF and PACF of residuals
par(mfrow = c(1, 2))
acf(as.numeric(na.omit(residuals(prophet_model)[,3][[1]])))
pacf(as.numeric(na.omit(residuals(prophet_model)[,3][[1]])))
```
The ACF and PACF plot indicate strong autocorrelation in the residuals.

```{r}
# ljung-box test on residuals
Box.test(residuals(prophet_model)[,3][[1]])
```
The p-value of the Ljung-box test is significant at the 95% confidence level, so we reject the null hypothesis and conclude that there is autocorrelation in the residuals, indicating a lack of fit of the model.

```{r}
# plot the CUSUM
plot(efp(residuals(prophet_model)[,3][[1]] ~ 1, type = "Rec-CUSUM"),
main = "Prophet CUSUM plot")
```
According to the CUSUM plot, the cumulative sum of the residuals have large deviations and fluctuations from 0, indicating a lack of fit of the model.

```{r}
# plot the 12 quarter ahead forecast
forecast(prophet_model, h = 12) %>%
  autoplot(ca_prices_tsibble) +
  ggtitle("Prophet model 12 step ahead forecast") +
  ylab("HPI")
```


### 6. Forecast Combination

```{r, message=FALSE}
auto_arima_model_train <- auto.arima(ca_price_train)
ets_model_train <- ets(ca_price_train)
holt_winters_model_train <- HoltWinters(ca_price_train)
nnetar_model_train <- nnetar(ca_price_train)
ca_prices_train_tsibble <- tsibble(Quarter = yearquarter(seq(as.Date("1975-01-01"), 
                                  as.Date("2009-10-01"), by = "quarter")),
        Price = ca_price_train)
prophet_model_train <- ca_prices_train_tsibble %>%
  model(prophet = prophet(Price ~ season(period = 4, order = 4,
                                    type = "additive")))


fore_auto_arima <- forecast(auto_arima_model_train, h = length(ca_price_test))$mean
fore_ets <- forecast(ets_model_train, h = length(ca_price_test))$mean
fore_holt_winters <- forecast(holt_winters_model_train, h = length(ca_price_test))$mean
fore_nnetar <- forecast(nnetar_model_train, h = length(ca_price_test))$mean
fore_prophet <- forecast(prophet_model_train, h = length(ca_price_test))[,4][[1]]

combine_df <- data.frame(
  Actual = ca_price_test,
  ARIMA = fore_auto_arima,
  ETS = fore_ets,
  HoltWinters = fore_holt_winters,
  NNETAR = fore_nnetar,
  Prophet = fore_prophet
  )

combined_model <- lm(Actual ~ 0 + ARIMA + ETS + HoltWinters + NNETAR, 
                     data = combine_df)

combined_weights <- coef(combined_model)

combined_fore_weighted <- 
  combined_weights["ARIMA"] * fore_auto_arima + 
  combined_weights["ETS"] * fore_ets + 
  combined_weights["HoltWinters"] * fore_holt_winters + 
  combined_weights["NNETAR"] * fore_nnetar

plot(ca_price_test, type = "l", col = "black", lwd = 2, 
     main = "Different Models and Combined Forecast",
     ylab = "HPI")
lines(fore_auto_arima, col = "blue", lty = 2)
lines(fore_ets, col = "red", lty = 2)
lines(fore_holt_winters, col = "green", lty = 2)
lines(fore_nnetar, col = "purple", lty = 2)
lines(combined_fore_weighted, col = "brown", lty = 2)
legend("topleft", legend = c("Actual", "ARIMA", "ETS", "Holt-Winters", "NNETAR", "Combined"),
       col = c("black", "blue", "red", "green", "purple", "brown"), lty = c(1,2,2,2,2,2), 
       lwd = c(2,1,1,1,1,1), cex = 0.9)
```
According to the plot of the fit of the individual models and combined forecasts, the combined forecasts (dotted brown line) fits the actual data (solid black line) the best. This indicates the combined forecast is preferred over the individual models. We will further verify this by looking at the error statistics of the models.

```{r}
# plot residuals vs fitted values
plot(as.numeric(combined_fore_weighted), as.numeric(ca_price_test - combined_fore_weighted), 
     main = "Mixed forecast test set residuals vs fitted values",
     xlab = "Fitted values",
     ylab = "Residuals")
```
There is a strong pattern and autocorrelation in the residuals, and the residuals are not evenly scattered around 0.

```{r}
par(mfrow = c(1, 2))
acf(as.numeric(ca_price_test - combined_fore_weighted))
pacf(as.numeric(ca_price_test - combined_fore_weighted))
```
The ACF and PACF plot indicate strong autocorrelation in the test set residuals.

```{r}
# ljung-box test on residuals
Box.test(ca_price_test - combined_fore_weighted)
```
The p-value of the Ljung-box test is significant at the 95% confidence level, so we reject the null hypothesis and conclude that there is autocorrelation in the residuals, indicating a lack of fit of the model.

```{r}
# plot the CUSUM
plot(efp((ca_price_test - combined_fore_weighted) ~ 1, type = "Rec-CUSUM"),
main = "Mixed forecast CUSUM plot")
```
According to the CUSUM plot, the cumulative sum of the residuals have large deviations and fluctuations from 0 (though not as large as the residuals in the Prophet model), indicating a lack of fit of the model.

### Training and Testing Error

```{r, message=FALSE}
# auto.arima model
accuracy(forecast(auto_arima_model_train, h = length(ca_price_test)), ca_price_test)

# ETS model
accuracy(forecast(ets_model_train, h = length(ca_price_test)), ca_price_test)

# Holt-Winters model
accuracy(forecast(holt_winters_model_train, h = length(ca_price_test)), ca_price_test)

# NNETAR model
accuracy(forecast(nnetar_model_train, h = length(ca_price_test)), ca_price_test)

# Prophet model
ca_prices_test_tsibble <- tsibble(Quarter = yearquarter(seq(as.Date("2010-01-01"), 
                                  as.Date("2024-10-01"), by = "quarter")),
        Price = ca_price_test)
accuracy(forecast(prophet_model_train, h = length(ca_price_test)), 
         ca_prices_test_tsibble)

# Combined model
accuracy(combined_fore_weighted, ca_price_test)
```
The errors in the test set of the combined forecast model are the lowest out of all the models and are significantly lower than the other model errors. This shows the combined forecat model is the preferred model based on lowest error. We focus on testing error because it shows the model performs well on unsees data and will have better forecasting performance, providing better real-world application of the data.

# III. Conclusions and Future Work

#### Conclusions

This study analyzed the quarterly House Price Index (HPI) of California from 1975 Q1 to 2024 Q2, employing various time series forecasting models, including ARIMA, ETS, Holt-Winters, NNETAR, Prophet, and a combined forecasting approach. The goal was to identify the most suitable model for predicting future house prices by evaluating model performance using diagnostic tests and error metrics.

The results indicate that the combined forecast model outperformed individual models in terms of minimizing prediction errors. The ARIMA model displayed strong residual properties, indicating white noise behavior, but its forecasting ability was constrained by its reliance on past linear patterns. The ETS and Holt-Winters models captured trend and seasonality effectively but had higher error rates. The NNETAR and Prophet models exhibited significant autocorrelation in residuals, suggesting inadequate fit for this dataset. The combined forecast model leveraged the strengths of multiple models, leading to the lowest testing errors and superior generalization capability.

Overall, the findings demonstrate that a hybrid approach of integrating multiple forecasting models can improve prediction accuracy and robustness, as it incorporates the strengths of each model. This insight is particularly valuable for stakeholders in the housing market, policymakers, and financial analysts who rely on accurate forecasts for decision-making.

#### Future Work

While the combined forecast model showed promising results, there is space for future research and improvement:

1. **Incorporation of External Variables:** Future studies could enhance the forecasting model by integrating macroeconomic indicators such as interest rates, inflation, employment rates, and mortgage rates to improve prediction accuracy.
   
2. **Refinement of Forecast Combination Methods:** The current combined forecasting approach assigns weights based on regression coefficients. Alternative techniques, such as directly taking the mean of the different model forecasts or weighted averaging based on model performance metrics, could be explored to further enhance predictive power.
   
4. **Comparative Analysis with Other States:** A cross-state analysis comparing California’s housing market trends with other high-cost states like New York or Texas could provide valuable insights into regional housing dynamics and price determinants.
   
5. **Impact of Policy Changes:** Future research could examine how government policies, such as tax incentives, housing subsidies, and zoning regulations, impact California’s housing market. Incorporating these factors into the forecasting model could provide insights into policy effectiveness and housing affordability.
   
By addressing these aspects, future research can contribute to a more comprehensive understanding of housing price dynamics and further improve predictive capabilities for economic decision-making.

# IV. References

https://www.fhfa.gov/data/hpi/datasets?tab=quarterly-data

https://www.fhfa.gov/hpi/download/quarterly_datasets/hpi_at_state.csv






